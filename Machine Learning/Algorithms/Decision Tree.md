# Decision Tree

## 模型介绍

&ensp;&ensp;&ensp;&ensp;决策树可以看成是一系列的if-then规则的集合。它训练的基本方法是：

![Dt](https://raw.githubusercontent.com/liuyaqiao/Learning-Note/master/DT.png)

&ensp;&ensp;&ensp;&ensp;决策树通常要进行递归地选择最优特征，并根据特征对数据进行分割，使得对各个子集有一个在当前条件下最好的分类。如果这些子集已经能够被基本正确分类，那么构建叶节点，并将这些子集分到所对应的叶节点去；如果还有子集不能正确分类，那么就对这些子集选择新的最有特征，继续对其进行分割，构建相应的结点。如此递归下去，直至所有训练数据自己被基本正确分类，或者没有合适的特征为止，最后每个子集都被分到叶节点上，即都有了明确的类，这就生成了一颗决策树。

&ensp;&ensp;&ensp;&ensp;如果发生过拟合，那么就要对树进行剪枝，让树变得更简单，去掉过于细分的叶节点，让其退化为父节点。

## 特征选择

&ensp;&ensp;&ensp;&ensp;对于一个集合，有很多特征都可以完成对集合的分割，我们需要寻找一个比较好的分割特征，或者说找一个评价特征好坏的标准。

- 信息增益  
&ensp;&ensp;&ensp;&ensp;是ID3采用的一种方法，它先定义了一个信息熵，它反应了数据的纯度。信息增益反应了纯度的提升，一般来说信息增益越大，利用属性a来划分数据得到的纯度提升越大，即这个属性越好。它的缺点在于，当某一个属性取值比较多的时候，它的增益率比较大，但这一个属性往往不是我们需要的。

- 信息增益比  
&ensp;&ensp;&ensp;&ensp;定义为信息增益对一个定值的商，是为了避免上面多取值带来的影响。但是这里又对取值较少的属性有所偏好。

- GINI系数  

&ensp;&ensp;&ensp;&ensp;直观来说，GINI系数反应了随机从数据集中抽取两个样本，他们不相同的概率。因此，GINI系数越小，则数据集D的纯度越高。所以我们要选择使得划分之后GINI系数最小的属性作为最优属性。

## 具体实现（ID3、C4.5、CART）

&ensp;&ensp;&ensp;&ensp;以CART为例：
&ensp;&ensp;&ensp;&ensp;CART同样由特征选取、树的生成和剪枝两部分组成，它既可以用于分类问题也可以用于回归问题。它以GINI系数最小化为准则，进行特征选择名，递归地生成二叉树。

- 回归树  
&ensp;&ensp;&ensp;&ensp;我们对输入空间进行一个划分，每一个划分对应了一个输出值。当输入空间确定之后，我们通过构造平方误差最小化的方式去确定输出值（一般来说是平均值）。
&ensp;&ensp;&ensp;&ensp;至于如何针对输入空间进行划分，我们使用最小二乘的方法，选择第i个变量和它的取的值并且求出当前分割的情况下，y的平方差最小的那个点（这里是每一个取值和当前区间中的均值作差），我们要求这两部分的和最小，当前点我们称为切割点。针对选定的切分变量，我们通过求平均值的办法求出这个区域的输出值。


- 分类树  
&ensp;&ensp;&ensp;&ensp;我们用GINI系数来选择最优特征，同时决定该特征的最优二值分割点（要遍历每一个特征和取值）。递归地调用GINI系数公式，取寻找那个GINI系数最小的特征，再进行下一步的生长，直到满足条件。  
&ensp;&ensp;&ensp;&ensp;之后我们采用后剪枝的方法，形成一个子树序列，之后取选择一个最优子树。


## 决策树的剪枝

- 预剪枝

&ensp;&ensp;&ensp;&ensp;预剪枝表示在决策树生成的过程中，对每一个结点在划分前先进行估计，如果当前结点不能带来决策树泛化性能的提升，则停止划分并把当前结点看作是叶节点。

- 后剪枝

&ensp;&ensp;&ensp;&ensp;后剪枝是指先从训练集中生成一颗完整的决策树，然后自底向上地对非叶结点进行考察，若将该结点对应的子树替换为叶结点能带来决策树泛化能力的提升，则将该结点替换成叶子结点。

&ensp;&ensp;&ensp;&ensp;预剪枝没有展开很多分支，虽然降低了过拟合风险和时间计算开销；但是，有些分支当前不能更好的泛化，但是之后可能会有更好的效果，所以会带来欠拟合的风险。后剪枝通常情况保留了更多的分支，欠拟合风险较小，泛化能力一般比较好。但是它需要等决策生完全生成之后在进行，并且是自底向上的，因此其训练开销比预剪枝大很多。

## 连续值与缺失值



