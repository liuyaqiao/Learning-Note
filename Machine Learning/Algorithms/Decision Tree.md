# Decision Tree

## 模型介绍

&ensp;&ensp;&ensp;&ensp;决策树可以看成是一系列的if-then规则的集合。它训练的基本方法是：

![Dt](https://raw.githubusercontent.com/liuyaqiao/Learning-Note/master/DT.png)

&ensp;&ensp;&ensp;&ensp;决策树通常要进行递归地选择最优特征，并根据特征对数据进行分割，使得对各个子集有一个在当前条件下最好的分类。如果这些子集已经能够被基本正确分类，那么构建叶节点，并将这些子集分到所对应的叶节点去；如果还有子集不能正确分类，那么就对这些子集选择新的最有特征，继续对其进行分割，构建相应的结点。如此递归下去，直至所有训练数据自己被基本正确分类，或者没有合适的特征为止，最后每个子集都被分到叶节点上，即都有了明确的类，这就生成了一颗决策树。

&ensp;&ensp;&ensp;&ensp;如果发生过拟合，那么就要对树进行剪枝，让树变得更简单，去掉过于细分的叶节点，让其退化为父节点。

## 特征选择

&ensp;&ensp;&ensp;&ensp;对于一个集合，有很多特征都可以完成对集合的分割，我们需要寻找一个比较好的分割特征，或者说找一个评价特征好坏的标准。

- 信息增益

- 信息增益比

- GINI系数

## 具体实现（ID3、C4.5、CART）

&ensp;&ensp;&ensp;&ensp;以CART为例：

## 决策树的剪枝

- 预剪枝

- 后剪枝

## 连续值与缺失值

