# 正则化

我们知道导致过拟合的原因主要是由于高次项的参数过大和参数过多导致的，所以我们分别通过这两方面去解决正则化，相关的可以参看[DL学习笔记](https://github.com/liuyaqiao/Graduate/blob/master/EECE5698_DL/Notes.md)和[LR](https://github.com/liuyaqiao/Learning-Note/blob/master/Machine%20Learning/Algorithms/Logistic%20Regression.md)

我们现在从三个角度来分别解释一下：

## 数学角度

### L2

如果加入了L2正则化之后的损失函数为:

<a href="https://www.codecogs.com/eqnedit.php?latex=L&space;=&space;L_0&space;&plus;&space;\frac{\lambda}{2}&space;\sum&space;w^2" target="_blank"><img src="https://latex.codecogs.com/gif.latex?L&space;=&space;L_0&space;&plus;&space;\frac{\lambda}{2}&space;\sum&space;w^2" title="L = L_0 + \frac{\lambda}{2} \sum w^2" /></a>

经过求导之后可得：

<a href="https://www.codecogs.com/eqnedit.php?latex=\frac{\partial&space;L}{\partial&space;w}&space;=&space;\frac{\partial&space;L_0}{\partial&space;w}&space;&plus;&space;\lambda&space;w" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\frac{\partial&space;L}{\partial&space;w}&space;=&space;\frac{\partial&space;L_0}{\partial&space;w}&space;&plus;&space;\lambda&space;w" title="\frac{\partial L}{\partial w} = \frac{\partial L_0}{\partial w} + \lambda w" /></a>

对w进行更新可以得到：

<a href="https://www.codecogs.com/eqnedit.php?latex=w&space;=&space;w&space;-&space;\eta&space;\frac{\partial&space;L}{\partial&space;w}&space;=&space;(1&space;-&space;\eta\lambda)w&space;-&space;\eta&space;\frac{\partial&space;L_0}{\partial&space;w}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?w&space;=&space;w&space;-&space;\eta&space;\frac{\partial&space;L}{\partial&space;w}&space;=&space;(1&space;-&space;\eta\lambda)w&space;-&space;\eta&space;\frac{\partial&space;L_0}{\partial&space;w}" title="w = w - \eta \frac{\partial L}{\partial w} = (1 - \eta\lambda)w - \eta \frac{\partial L_0}{\partial w}" /></a>

我们得到了一个乘法因子去调整权重，这回影响它的下降曲线，可以使他变得更小，得到一定程度的限制的效果。可以根据w的正负调整速度，起到限制的效果。

我们也从数学角度来看一下L2，它是加入了一个平方项。正则项会和之前的误差一起作用起到最小化的效果。如果w增大是l变小，但是正则项就会增大，这样就会起到一个限制的作用。即使最初的设置是很复杂的函数，最后得出的模型高次方项也会很小。

### L0、L1

至于L0和L1会使w变得稀疏，我们可以这样理解：

1. L0范数指的是向量中非零元素的个数，L0正则化就是把非零元素的个数限制在一定的范围之内，这会带来稀疏性。但是这是一个NP-hard问题。所以用L1正则化代替L0正则化。

2. L1正则化：

<a href="https://www.codecogs.com/eqnedit.php?latex=L&space;=&space;L_0&space;&plus;&space;\lambda&space;\sum&space;|w|" target="_blank"><img src="https://latex.codecogs.com/gif.latex?L&space;=&space;L_0&space;&plus;&space;\lambda&space;\sum&space;|w|" title="L = L_0 + \lambda \sum |w|" /></a>

求导之后可得：

<a href="https://www.codecogs.com/eqnedit.php?latex=\frac{\partial&space;L}{\partial&space;w}&space;=&space;\frac{\partial&space;L_0}{\partial&space;w}&space;&plus;&space;\lambda&space;\cdot&space;sgn(w)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\frac{\partial&space;L}{\partial&space;w}&space;=&space;\frac{\partial&space;L_0}{\partial&space;w}&space;&plus;&space;\lambda&space;\cdot&space;sgn(w)" title="\frac{\partial L}{\partial w} = \frac{\partial L_0}{\partial w} + \lambda \cdot sgn(w)" /></a>

<a href="https://www.codecogs.com/eqnedit.php?latex=w&space;=&space;w&space;-&space;\eta\frac{\partial&space;L_0}{\partial&space;w}&space;=&space;w&space;-&space;\eta&space;\lambda&space;\cdot&space;sgn(w)&space;-&space;\eta&space;\frac{\partial&space;L_0}{\partial&space;w}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?w&space;=&space;w&space;-&space;\eta\frac{\partial&space;L_0}{\partial&space;w}&space;=&space;w&space;-&space;\eta&space;\lambda&space;\cdot&space;sgn(w)&space;-&space;\eta&space;\frac{\partial&space;L_0}{\partial&space;w}" title="w = w - \eta\frac{\partial L_0}{\partial w} = w - \eta \lambda \cdot sgn(w) - \eta \frac{\partial L_0}{\partial w}" /></a>

可以看出，我们通过减去一个常量的方式让w向0靠近；对比L2，当|w|很大时,L2对权重的衰减速度比L1大得多，当|w|很小时，L1对权重的缩小比L2快得多。

这也就解释了为什么L1正则能让模型变得稀疏。L1对于小权重减小地很快，对大权重减小较慢，因此最终模型的权重主要集中在那些高重要度的特征上，对于不重要的特征，权重会很快趋近于0。所以最终权重w会变得稀疏。

## 先验角度

正则化等价于对模型参数引入了先验分布，L1 Regularization 是认为模型参数服从拉普拉斯分布，L2 Regularization则认为模型参数服从高斯分布（这里为了方便，使用线性回归的误差函数来说明）。

其公式为:  

<a href="https://www.codecogs.com/eqnedit.php?latex=f(x)&space;=&space;\frac{1}{\sigma&space;\sqrt{2\pi&space;}}e^{-\frac{(x&space;-&space;\mu&space;)^{2}}{2\sigma^{2}}}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?f(x)&space;=&space;\frac{1}{\sigma&space;\sqrt{2\pi&space;}}e^{-\frac{(x&space;-&space;\mu&space;)^{2}}{2\sigma^{2}}}" title="f(x) = \frac{1}{\sigma \sqrt{2\pi }}e^{-\frac{(x - \mu )^{2}}{2\sigma^{2}}}" /></a>  

我们通过去对数似然函数的方法，可以分离出一个平方项。这就是L2的来源。而针对L1的数据，他们遵循拉普拉斯分布。

<a href="https://www.codecogs.com/eqnedit.php?latex=f(x)&space;=&space;\frac{1}{2\lambda&space;}e^{-\frac{\left&space;|&space;x&space;-&space;\mu&space;\right&space;|}{\lambda&space;}}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?f(x)&space;=&space;\frac{1}{2\lambda&space;}e^{-\frac{\left&space;|&space;x&space;-&space;\mu&space;\right&space;|}{\lambda&space;}}" title="f(x) = \frac{1}{2\lambda }e^{-\frac{\left | x - \mu \right |}{\lambda }}" /></a>  

![Laplace](https://raw.githubusercontent.com/liuyaqiao/Learning-Note/master/laplace.jpg)  
拉普拉斯分布所得到的对数似然函数的附加项是线性，也就是L1的形式。  

数学推导：

规则化 = 加入参数的先验信息  
我们认为w满足拉普拉斯分布或者高斯分布，不是单纯的天马行空的取w的值，而是有一个限制。  

<a href="https://www.codecogs.com/eqnedit.php?latex=L(w)&space;=&space;p(y|X;w)p(w)=\prod_{i&space;=&space;1}^{m}p(y^{i}|x^{i};\theta)p(w)&space;\\=&space;\prod_{i&space;=&space;1}^{m}&space;\frac{1}{\sqrt{2\pi}\delta&space;}&space;exp(-\frac{(y^{i}&space;-&space;w^{T}x^{i})^{2}}{2\delta^{2}})&space;\frac{1}{\sqrt{2\pi}\alpha&space;}&space;exp(-&space;\frac{w^{T}w}{2\alpha&space;})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?L(w)&space;=&space;p(y|X;w)p(w)=\prod_{i&space;=&space;1}^{m}p(y^{i}|x^{i};\theta)p(w)&space;\\=&space;\prod_{i&space;=&space;1}^{m}&space;\frac{1}{\sqrt{2\pi}\delta&space;}&space;exp(-\frac{(y^{i}&space;-&space;w^{T}x^{i})^{2}}{2\delta^{2}})&space;\frac{1}{\sqrt{2\pi}\alpha&space;}&space;exp(-&space;\frac{w^{T}w}{2\alpha&space;})" title="L(w) = p(y|X;w)p(w)=\prod_{i = 1}^{m}p(y^{i}|x^{i};\theta)p(w) \\= \prod_{i = 1}^{m} \frac{1}{\sqrt{2\pi}\delta } exp(-\frac{(y^{i} - w^{T}x^{i})^{2}}{2\delta^{2}}) \frac{1}{\sqrt{2\pi}\alpha } exp(- \frac{w^{T}w}{2\alpha })" /></a>  

两边取对数之后可以得到：  
    
<a href="https://www.codecogs.com/eqnedit.php?latex=l(w)&space;=&space;logL(w)&space;\\=mlog\frac{1}{\sqrt{2\pi&space;}\delta&space;}&space;&plus;&space;n&space;log&space;\frac{1}{\sqrt{2\pi&space;\alpha&space;}}&space;-&space;\frac{1}{\delta&space;^{2}}&space;\cdot&space;\frac{1}{2}&space;\sum_{i&space;=&space;1}^{m}&space;(y^{i}&space;-&space;w^{T}&space;x^{i})&space;^{2}&space;-&space;\frac{1}{\alpha}&space;\cdot&space;\frac{1}{2}&space;w^{T}w" target="_blank"><img src="https://latex.codecogs.com/gif.latex?l(w)&space;=&space;logL(w)&space;\\=mlog\frac{1}{\sqrt{2\pi&space;}\delta&space;}&space;&plus;&space;n&space;log&space;\frac{1}{\sqrt{2\pi&space;\alpha&space;}}&space;-&space;\frac{1}{\delta&space;^{2}}&space;\cdot&space;\frac{1}{2}&space;\sum_{i&space;=&space;1}^{m}&space;(y^{i}&space;-&space;w^{T}&space;x^{i})&space;^{2}&space;-&space;\frac{1}{\alpha}&space;\cdot&space;\frac{1}{2}&space;w^{T}w" title="l(w) = logL(w) \\=mlog\frac{1}{\sqrt{2\pi }\delta } + n log \frac{1}{\sqrt{2\pi \alpha }} - \frac{1}{\delta ^{2}} \cdot \frac{1}{2} \sum_{i = 1}^{m} (y^{i} - w^{T} x^{i}) ^{2} - \frac{1}{\alpha} \cdot \frac{1}{2} w^{T}w" /></a>  
   
等价于： 
   
 <a href="https://www.codecogs.com/eqnedit.php?latex=w_{MAP}&space;=&space;argmin(\frac{1}{\delta^{2}}&space;\cdot&space;\frac{1}{2}&space;\sum_{i&space;=&space;1}^{m}&space;(y^{i}&space;-&space;w^{T}x^{i})&space;^{2}&space;&plus;&space;\frac{1}{\alpha}&space;\cdot&space;\frac{1}{2}&space;w^{T}w)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?w_{MAP}&space;=&space;argmin(\frac{1}{\delta^{2}}&space;\cdot&space;\frac{1}{2}&space;\sum_{i&space;=&space;1}^{m}&space;(y^{i}&space;-&space;w^{T}x^{i})&space;^{2}&space;&plus;&space;\frac{1}{\alpha}&space;\cdot&space;\frac{1}{2}&space;w^{T}w)" title="w_{MAP} = argmin(\frac{1}{\delta^{2}} \cdot \frac{1}{2} \sum_{i = 1}^{m} (y^{i} - w^{T}x^{i}) ^{2} + \frac{1}{\alpha} \cdot \frac{1}{2} w^{T}w)" /></a>  
    
即可等价于：  
    
 <a href="https://www.codecogs.com/eqnedit.php?latex=J_{R}(w)&space;=&space;\frac{1}{n}\left&space;\|&space;y&space;-&space;w^{T}X&space;\right&space;\|^{2}&space;&plus;&space;\lambda&space;\left&space;\|&space;w&space;\right&space;\|^{2}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?J_{R}(w)&space;=&space;\frac{1}{n}\left&space;\|&space;y&space;-&space;w^{T}X&space;\right&space;\|^{2}&space;&plus;&space;\lambda&space;\left&space;\|&space;w&space;\right&space;\|^{2}" title="J_{R}(w) = \frac{1}{n}\left \| y - w^{T}X \right \|^{2} + \lambda \left \| w \right \|^{2}" /></a>  
    
这就是我们所说的Ridge Regression，增加了L2正则化之后的损失函数的形式。类似的，如果设定w满足拉普拉斯分布，则cost function会有一个线性项的形式。拉普拉斯分布的特殊形式会为参数带来稀疏化的优良特性。  

先验分布为高斯分布，对于损失函数的L2正则化项，先验分布为拉普拉斯分布，对应损失函数的L1正则化项。

## 图形角度

![](https://raw.githubusercontent.com/liuyaqiao/Learning-Note/master/regularization.png)  
途中实线为参数w的等值线，虚线是L函数的图线。他们第一次相交的地方是最有解存在的取值。

从上图中可以看到，L1的的两条线大部分会在矩形角的位置相交。这个位置会有w的分量为0。试想，在高维w分量的时候，就会出现多个w分量为0的情况。从而导致稀疏矩阵的出现。而L2正则化就没有这样的性质，所以不会有太多的稀疏性出现。

## 结构风险最小化

机器学习的魔都是为了让期望风险最小化，但是实际上我们最小化的是经验风险。

<a href="https://www.codecogs.com/eqnedit.php?latex=R_{exp}&space;=&space;E_p[L(T,&space;f(X))]&space;=&space;\int&space;L(y,&space;f(x))P(x,y)dxdy" target="_blank"><img src="https://latex.codecogs.com/gif.latex?R_{exp}&space;=&space;E_p[L(T,&space;f(X))]&space;=&space;\int&space;L(y,&space;f(x))P(x,y)dxdy" title="R_{exp} = E_p[L(T, f(X))] = \int L(y, f(x))P(x,y)dxdy" /></a>

期望风险是模型关于联合分布的期望损失，经验风险是模型关于训练样本集的平均损失，当N趋近于无穷时，经验风险趋近于期望风险，因此我们可以用经验风险最小化去近似期望风险最小化。

<a href="https://www.codecogs.com/eqnedit.php?latex=R_{emp}(f)&space;=&space;\frac{1}{N}&space;\sum_{i}^N&space;L(y_i,&space;f(x_i))" target="_blank"><img src="https://latex.codecogs.com/gif.latex?R_{emp}(f)&space;=&space;\frac{1}{N}&space;\sum_{i}^N&space;L(y_i,&space;f(x_i))" title="R_{emp}(f) = \frac{1}{N} \sum_{i}^N L(y_i, f(x_i))" /></a>

然而，我们的训练集样本数量N很有限，用经验风险估计近似期望风险往往不理想，容易产生“过拟合”的现象，因此需要使用结构风险来矫正经验风险。

结构风险(structural risk)最小化的目的是防止过拟合，它是在经验风险上加上表示模型复杂度的正则化项：

<a href="https://www.codecogs.com/eqnedit.php?latex=R_{srm}(f)&space;=&space;\frac{1}{N}&space;\sum_{i}^N&space;L(y_i,&space;f(x_i))&space;&plus;&space;\lambda&space;J(f)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?R_{srm}(f)&space;=&space;\frac{1}{N}&space;\sum_{i}^N&space;L(y_i,&space;f(x_i))&space;&plus;&space;\lambda&space;J(f)" title="R_{srm}(f) = \frac{1}{N} \sum_{i}^N L(y_i, f(x_i)) + \lambda J(f)" /></a>

其中J(f)为模型的复杂度。结构风险最小化的目标就是让经验风险与模型复杂度同时小。监督学习的问题实际上也就是经验风险或结构风险最小化问题，对于样本容量大的情况，经验风险最小化可以达到很好的效果，例如极大似然估计；当样本容量不够时，需要使用结构风险最小化来避免过拟合，例如最大后验概率估计，其模型的复杂度由模型的先验概率表示。

